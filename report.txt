**House Keeping**
Timing reports are output from the Measure-Command utility on windows 11, these are in the timings folder.
GuitarDry.wav and bigHall.wav were used for all timings.

The baseline version can be found in convolveBaseline.cpp
Baseline Version Time: 823738.9601ms

**Optimizations**
The first optimization I did was an algorithm-based optimization.
I chose to use the overlap-add algorithm from p.311-318 in the Smith text, and I used the four1 FFT that we were given.
this version can be found at https://github.com/E-Gantz/501A4/blob/4d924f5712a7861c261599857af767ff1202c9a5/convolve.cpp
Algorithm optimized time: 3542.7792ms

Next I started using a profiler to see where I could do some manual tuning.
To do this I used gprof and compiled with "g++ -Wall -pg -no-pie -o convolve convolve.cpp"
    note: the -no-pie flag is there because I was getting empty gprof output which is apparently a gcc bug: https://stackoverflow.com/questions/42620074/gprof-produces-empty-output
At this point I also started using the diff command to do regression testing to compare output files.

The gprof output claimed that 75% of output time was in four1, which I was too scared to touch, so I just looked through the code to find things to optimize.
My first manual tuning was to remove unnecessary code.
First I found that in sampleToDouble I was checking if the input value was negative and doing an absolute value calculation, so i just replaced that with "return double(value) / (double)32767.0;" to cut down on if statement calls and unnecessary operations.
note that this method is run from a loop, so this effectively 'unswitches' the loop by removing if statements within the loop.
I also removed the final print statement, as it was unnecessary I/O, and the zeroPadding method as i had already inlined it.
this version can be found at https://github.com/E-Gantz/501A4/blob/ea405d43946d6e87c9a36f0ffac8fb2f53d58cd9/convolve.cpp
manual tuning time: 1503.1768ms

Next I looked for other functions I could inline, but found none, and values that were being recalculated that I could cache.
The most obvious one was M-1, so I cached that to use throughout my convolve method.
I also found that I was using 2*paddingSize a lot which not only is a repeated operation, but a repeated multiplication which is extra smelly, so I cached that as well.
note that by 'cached' I mean U put the result in a variable and used the variable instead of the calculation. I'm not sure if you can change the system cache in C++ in a similar way to manually using the registers, but I didn't do anything of that sort.
Another optimization I had done earlier was changing from tracking the position in the input and output vectors via an offset variable (i*number of samples per segment, i goes from 0 to number of segments-1) to 
just tracking the position using the inputPos and outputPos variables, which removes one multiplication per loop.
this version can be found at https://github.com/E-Gantz/501A4/blob/9c4f2d991b5313df9b19c18216534599104b0905/convolve.cpp
manual tuning time: 1434.0383ms

I then noticed I had a lot of little for loops within the convolve function, and looked for some that I could jam together.
I had two consecutive "for (int j=0; j<paddingSize; j++)" loops, one to fill the output vector from the real part of the post IFFT vector, and one to scale the values, so I combined these into one loop.
at this point four1 had gone up to over 80% of the run time according to gprof, which shows that I increased the speed of the other parts.
manual tuning time: 1322.2385ms